{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "\n",
    "data = pd.read_csv(r'C:\\Users\\Grade A\\Desktop\\DATASET\\data science africa\\Python-Data-Basics\\dataset_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.147868</td>\n",
       "      <td>0.937782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.173189</td>\n",
       "      <td>-0.264483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.680447</td>\n",
       "      <td>0.048147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.088036</td>\n",
       "      <td>1.028807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.193379</td>\n",
       "      <td>-1.155007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  label\n",
       "0   1.147868   0.937782      1\n",
       "1   0.173189  -0.264483      0\n",
       "2   0.680447   0.048147      1\n",
       "3  -1.088036   1.028807      1\n",
       "4   1.193379  -1.155007      0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the first five rows of the dataset\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting & dropping the target varaiable into X & y\n",
    "\n",
    "X = data.drop('label', axis=1)\n",
    "y = data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling in the (AUC) function\n",
    "\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling in the (AUC) function\n",
    "\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning my model\n",
    "\n",
    "model=CatBoostClassifier(n_estimators=800,eval_metric='AUC',max_depth=5,learning_rate=0.1,od_wait=50, \n",
    "                              subsample=0.9,bootstrap_type='Bernoulli',metric_period=20,\n",
    "                     #l2_leaf_reg=5,#bagging_temperature=0.85,random_strength=100,\n",
    "                     use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.9374213\tbest: 0.9374213 (0)\ttotal: 5.11ms\tremaining: 4.08s\n",
      "20:\ttest: 0.9736558\tbest: 0.9751843 (17)\ttotal: 84.9ms\tremaining: 3.15s\n",
      "40:\ttest: 0.9717677\tbest: 0.9751843 (17)\ttotal: 157ms\tremaining: 2.9s\n",
      "60:\ttest: 0.9696098\tbest: 0.9751843 (17)\ttotal: 238ms\tremaining: 2.88s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9751843194\n",
      "bestIteration = 17\n",
      "\n",
      "Shrink model to first 18 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x14e122b1808>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting my model\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving my prediction file\n",
    "\n",
    "pred = model.predict_proba(data)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting my model for further tuning\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.9655373\ttest1: 0.9627093\tbest: 0.9627093 (0)\ttotal: 6.36ms\tremaining: 5.08s\n",
      "100:\ttest: 0.9981693\ttest1: 0.9852092\tbest: 0.9859981 (19)\ttotal: 358ms\tremaining: 2.47s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9859980638\n",
      "bestIteration = 19\n",
      "\n",
      "Shrink model to first 20 iterations.\n",
      "err:  0.985998063752734\n",
      "0:\ttest: 0.9696013\ttest1: 0.9434745\tbest: 0.9434745 (0)\ttotal: 4.21ms\tremaining: 3.37s\n",
      "100:\ttest: 0.9975275\ttest1: 0.9908376\tbest: 0.9908376 (99)\ttotal: 388ms\tremaining: 2.69s\n",
      "200:\ttest: 0.9998292\ttest1: 0.9907294\tbest: 0.9909097 (110)\ttotal: 789ms\tremaining: 2.35s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9909097468\n",
      "bestIteration = 110\n",
      "\n",
      "Shrink model to first 111 iterations.\n",
      "err:  0.9909097467715171\n",
      "0:\ttest: 0.9588391\ttest1: 0.9783746\tbest: 0.9783746 (0)\ttotal: 4.43ms\tremaining: 3.54s\n",
      "100:\ttest: 0.9980714\ttest1: 0.9879879\tbest: 0.9899358 (29)\ttotal: 380ms\tremaining: 2.63s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9899357911\n",
      "bestIteration = 29\n",
      "\n",
      "Shrink model to first 30 iterations.\n",
      "err:  0.9899357910684655\n"
     ]
    }
   ],
   "source": [
    "# my first tuning model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\n",
    "errcb=[]\n",
    "y_pred_totcb=[]\n",
    "from sklearn.model_selection import KFold,StratifiedKFold, TimeSeriesSplit, GroupKFold\n",
    "fold=StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "i=1\n",
    "for train_index, test_index in fold.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    m=CatBoostClassifier(n_estimators=800,eval_metric='AUC',max_depth=4,learning_rate=0.1,#reg_lambda=5,#5\n",
    "                              subsample=0.9,bootstrap_type='Bernoulli',#leaf_estimation_iterations=10,\n",
    "                    #l2_leaf_reg=5,#bagging_temperature=0.85,random_strength=100,\n",
    "                     use_best_model=True)\n",
    "    #CatBoostClassifier(n_estimators=1000,eval_metric='AUC',max_depth=5,learning_rate=0.1,reg_lambda=5,#5\n",
    "                              #subsample=0.9,bootstrap_type='Bernoulli',\n",
    "                    #l2_leaf_reg=5,#bagging_temperature=0.85,random_strength=100,\n",
    "                     #use_best_model=True)\n",
    "    m.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=100,verbose=100)\n",
    "    preds=m.predict_proba(X_test)[:, 1]\n",
    "    print(\"err: \",roc_auc_score(y_test,preds))\n",
    "    errcb.append(roc_auc_score(y_test,preds))\n",
    "    p = m.predict_proba(data)[:, 1]\n",
    "    y_pred_totcb.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889478671975721"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the np.mean(errb)\n",
    "\n",
    "\n",
    "np.mean(errcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.9726751\ttest1: 0.9657930\tbest: 0.9657930 (0)\ttotal: 4.41ms\tremaining: 4.41s\n",
      "100:\ttest: 0.9979799\ttest1: 0.9842052\tbest: 0.9873606 (3)\ttotal: 525ms\tremaining: 4.67s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9873606081\n",
      "bestIteration = 3\n",
      "\n",
      "Shrink model to first 4 iterations.\n",
      "err:  0.9873606081250672\n",
      "0:\ttest: 0.9771943\ttest1: 0.9672282\tbest: 0.9672282 (0)\ttotal: 4.56ms\tremaining: 4.55s\n",
      "100:\ttest: 0.9969251\ttest1: 0.9887995\tbest: 0.9889077 (93)\ttotal: 518ms\tremaining: 4.61s\n",
      "200:\ttest: 0.9994785\ttest1: 0.9905310\tbest: 0.9908556 (198)\ttotal: 975ms\tremaining: 3.88s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9908556381\n",
      "bestIteration = 198\n",
      "\n",
      "Shrink model to first 199 iterations.\n",
      "err:  0.9908556381213477\n",
      "0:\ttest: 0.9739080\ttest1: 0.9758856\tbest: 0.9758856 (0)\ttotal: 4.5ms\tremaining: 4.5s\n",
      "100:\ttest: 0.9974960\ttest1: 0.9874468\tbest: 0.9902063 (33)\ttotal: 522ms\tremaining: 4.65s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9902063343\n",
      "bestIteration = 33\n",
      "\n",
      "Shrink model to first 34 iterations.\n",
      "err:  0.9902063343193133\n"
     ]
    }
   ],
   "source": [
    "#checking the second tuning model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\n",
    "errcb1=[]\n",
    "y_pred_totcb1=[]\n",
    "from sklearn.model_selection import KFold,StratifiedKFold, TimeSeriesSplit\n",
    "fold=StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "i=1\n",
    "for train_index, test_index in fold.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    m1=CatBoostClassifier(n_estimators=1000,eval_metric='AUC',max_depth=4,learning_rate=0.1,od_wait=50, reg_lambda=3,\n",
    "                              bootstrap_type='Bayesian',\n",
    "                     use_best_model=True)\n",
    "    m1.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=100,verbose=100)\n",
    "    preds=m1.predict_proba(X_test)[:, 1]\n",
    "    print(\"err: \",roc_auc_score(y_test,preds))\n",
    "    errcb1.append(roc_auc_score(y_test,preds))\n",
    "    p1 = m1.predict_proba(data)[:, 1]\n",
    "    y_pred_totcb1.append(p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894741935219095"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the np.mean(errcb1)\n",
    "\n",
    "np.mean(errcb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savign my tuned model into test_predictions\n",
    "\n",
    "d = {\"label\": np.mean(y_pred_totcb, 0)}\n",
    "test_predictions = pd.DataFrame(data=d)\n",
    "test_predictions = test_predictions[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.to_csv('mode.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.732201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.988941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "0  0.992398\n",
       "1  0.122025\n",
       "2  0.732201\n",
       "3  0.988941\n",
       "4  0.011037"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the first five rows of my test_predictions\n",
    "\n",
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving my second tuned model\n",
    "\n",
    "d = {\"label\": np.mean(y_pred_totcb1, 0)}\n",
    "test_predictioned = pd.DataFrame(data=d)\n",
    "test_predictioned = test_predictioned[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictioned.to_csv('mode2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.927316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.191907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.772954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.916195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "0  0.927316\n",
       "1  0.191907\n",
       "2  0.772954\n",
       "3  0.916195\n",
       "4  0.080720"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the first five rows of my test_predictioned\n",
    "\n",
    "test_predictioned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving my predictions into a & b for test_predictions and test_predictioned respectively\n",
    "\n",
    "a = pd.read_csv('mode.csv')\n",
    "b = pd.read_csv('mode2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987253252202769"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coreelating my predictions\n",
    "\n",
    "a['label'].corr(b['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987253252202769"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlating my predictions\n",
    "\n",
    "test_predictions['label'].corr(test_predictioned['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blendin my models intp test_predictions\n",
    "\n",
    "test_predictions['label'] = (a['label'] * 0.69 + b['label'] * 0.35) * round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking my model\n",
    "\n",
    "test_predictions.to_csv('data science africa_stack1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving my prediction into data.to_csv\n",
    "\n",
    "data.to_csv('data science africa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving my prediction file\n",
    "pred = model.predict_proba(data)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
